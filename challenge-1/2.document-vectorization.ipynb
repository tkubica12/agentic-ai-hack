{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vectorization with Azure AI Search Integrated Vectorization\n",
    "\n",
    "This notebook demonstrates how to create a sophisticated search index using Azure AI Search's integrated vectorization capabilities for insurance document retrieval. The workflow includes:\n",
    "\n",
    "1. **Retrieve Processed Documents from Azure Blob Storage**: Download the processed insurance documents (policies, claims, statements) that were created in the previous notebook, including structured claim data and detailed image descriptions.\n",
    "\n",
    "2. **Create Azure AI Search Index with Integrated Vectorization**: \n",
    "   - **Index Schema Design**: Define a comprehensive search schema with fields for content, metadata, and vector embeddings\n",
    "   - **Integrated Vectorization Setup**: Configure Azure AI Search to automatically generate embeddings using Azure OpenAI's text-embedding-ada-002 model\n",
    "   - **Semantic Search Configuration**: Enable semantic search capabilities for natural language queries\n",
    "\n",
    "3. **Intelligent Text Chunking**: Process large insurance documents into optimally-sized chunks with overlapping content to ensure comprehensive coverage while maintaining context for accurate retrieval.\n",
    "\n",
    "4. **Upload Documents to Azure AI Search**: \n",
    "   - **Batch Processing**: Efficiently upload document chunks to the search index\n",
    "   - **Automatic Embedding Generation**: Azure AI Search automatically creates vector embeddings for each document chunk using the configured OpenAI model\n",
    "   - **Real-time Indexing**: Documents become immediately searchable upon upload\n",
    "\n",
    "5. **Advanced Search Testing**: \n",
    "   - **Semantic Search**: Test natural language queries against insurance policies using AI-powered semantic understanding\n",
    "   - **Vector Search**: Perform similarity-based searches using vector embeddings\n",
    "   - **Hybrid Search**: Combine keyword and vector search for optimal results\n",
    "   - **Interactive Testing**: Provide an interactive interface for real-time search testing\n",
    "\n",
    "6. **Search Analytics and Validation**: Generate comprehensive statistics about the indexed documents, search performance, and readiness for AI agent integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "Let's start with handling the import of our libraries and load the `.env` variables that we have saved in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    ExhaustiveKnnAlgorithmConfiguration\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "üîç Search Service: msagthack-search-m3qz57ik6ngog\n",
      "üîó Search Endpoint: https://msagthack-search-m3qz57ik6ngog.search.windows.net/\n",
      "üì¶ Processed Documents Container: processed-documents\n",
      "üìá Search Index: insurance-documents-index\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    \n",
    "    # Azure AI Search configuration\n",
    "    SEARCH_SERVICE_NAME = os.getenv('SEARCH_SERVICE_NAME')\n",
    "    SEARCH_SERVICE_ENDPOINT = os.getenv('SEARCH_SERVICE_ENDPOINT')\n",
    "    SEARCH_ADMIN_KEY = os.getenv('SEARCH_ADMIN_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration (for integrated vectorization)\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT', 'text-embedding-ada-002')\n",
    "    \n",
    "    # Container names\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    \n",
    "    # Search index configuration\n",
    "    SEARCH_INDEX_NAME = 'insurance-documents-index'\n",
    "    CHUNK_SIZE = 1000  # Characters per chunk\n",
    "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.SEARCH_SERVICE_ENDPOINT,\n",
    "    Config.SEARCH_ADMIN_KEY,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables:\")\n",
    "    if not Config.SEARCH_SERVICE_ENDPOINT:\n",
    "        print(\"  - SEARCH_SERVICE_ENDPOINT\")\n",
    "    if not Config.SEARCH_ADMIN_KEY:\n",
    "        print(\"  - SEARCH_ADMIN_KEY\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"üîç Search Service: {Config.SEARCH_SERVICE_NAME}\")\n",
    "    print(f\"üîó Search Endpoint: {Config.SEARCH_SERVICE_ENDPOINT}\")\n",
    "    print(f\"üì¶ Processed Documents Container: {Config.PROCESSED_CONTAINER}\")\n",
    "    print(f\"üìá Search Index: {Config.SEARCH_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Azure Services\n",
    "\n",
    "The next cell creates connections to Azure Blob Storage for document retrieval and Azure AI Search for index management, with comprehensive error handling and connection testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Blob Storage - Found 4 containers\n",
      "‚úÖ Connected to Azure AI Search - Storage used: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure AI Search clients\n",
    "        search_credential = AzureKeyCredential(Config.SEARCH_ADMIN_KEY)\n",
    "        \n",
    "        search_index_client = SearchIndexClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        search_client = SearchClient(\n",
    "            endpoint=Config.SEARCH_SERVICE_ENDPOINT,\n",
    "            index_name=Config.SEARCH_INDEX_NAME,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        # Test the connections\n",
    "        containers = list(blob_service_client.list_containers())\n",
    "        print(f\"‚úÖ Connected to Blob Storage - Found {len(containers)} containers\")\n",
    "        \n",
    "        # Test search service (fixed the storage_size access)\n",
    "        try:\n",
    "            service_stats = search_index_client.get_service_statistics()\n",
    "            storage_used = getattr(service_stats, 'storage_size', 'Unknown')\n",
    "            print(f\"‚úÖ Connected to Azure AI Search - Storage used: {storage_used}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úÖ Connected to Azure AI Search - Service is available\")\n",
    "            print(f\"   (Note: Could not get statistics: {e})\")\n",
    "        \n",
    "        return blob_service_client, search_index_client, search_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing clients: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize clients\n",
    "blob_service_client, search_index_client, search_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Azure AI Search Index with Integrated Vectorization\n",
    "The next cell defines a `SearchIndexManager` class that creates a sophisticated search index with integrated vectorization, semantic search capabilities, and proper field schema for insurance documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Original endpoint: https://msagthack-aifoundry-m3qz57ik6ngog.cognitiveservices.azure.com/\n",
      "üîó Formatted endpoint: https://msagthack-aifoundry-m3qz57ik6ngog.openai.azure.com\n",
      "üöÄ Using deployment: text-embedding-ada-002\n",
      "‚úÖ Search index 'insurance-documents-index' created successfully!\n",
      "üìã Index fields: 12\n",
      "üîç Vector search enabled: True\n",
      "üß† Semantic search enabled: True\n",
      "\n",
      "üìä Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "class SearchIndexManager:\n",
    "    \"\"\"Class to manage Azure AI Search index with integrated vectorization\"\"\"\n",
    "    \n",
    "    def __init__(self, search_index_client: SearchIndexClient):\n",
    "        self.search_index_client = search_index_client\n",
    "        self.index_name = Config.SEARCH_INDEX_NAME\n",
    "\n",
    "    def _format_azure_openai_endpoint(self, endpoint: str) -> str:\n",
    "        \"\"\"Format the Azure OpenAI endpoint for use with Azure AI Search vectorizer\"\"\"\n",
    "        # Remove trailing slash if present\n",
    "        endpoint = endpoint.rstrip('/')\n",
    "        \n",
    "        # Check if it already has the correct format\n",
    "        if endpoint.endswith('.openai.azure.com'):\n",
    "            return endpoint\n",
    "        \n",
    "        # Extract the resource name from various possible formats\n",
    "        if '.cognitiveservices.azure.com' in endpoint:\n",
    "            # Convert from cognitive services format to OpenAI format\n",
    "            resource_name = endpoint.split('.')[0].split('//')[-1]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        elif '/openai/' in endpoint:\n",
    "            # Extract resource name from URL with /openai/ path\n",
    "            parts = endpoint.split('/')\n",
    "            resource_name = parts[2].split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "        else:\n",
    "            # Try to extract resource name and format correctly\n",
    "            if 'https://' in endpoint:\n",
    "                resource_name = endpoint.split('//')[1].split('.')[0]\n",
    "            else:\n",
    "                resource_name = endpoint.split('.')[0]\n",
    "            return f\"https://{resource_name}.openai.azure.com\"\n",
    "    \n",
    "    def create_search_index(self) -> bool:\n",
    "        \"\"\"Create a search index with integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Format the Azure OpenAI endpoint correctly\n",
    "            formatted_endpoint = self._format_azure_openai_endpoint(Config.AZURE_OPENAI_ENDPOINT)\n",
    "            print(f\"üîó Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"üîó Formatted endpoint: {formatted_endpoint}\")\n",
    "            print(f\"üöÄ Using deployment: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            \n",
    "            # Define the vectorizer for integrated vectorization\n",
    "            vectorizer = AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"insurance-vectorizer\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=formatted_endpoint,  # Use formatted endpoint\n",
    "                    deployment_name=Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,  # Use config variable\n",
    "                    model_name=\"text-embedding-ada-002\",\n",
    "                    api_key=Config.AZURE_OPENAI_API_KEY\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Define vector search configuration\n",
    "            vector_search = VectorSearch(\n",
    "                algorithms=[\n",
    "                    HnswAlgorithmConfiguration(name=\"insurance-algorithm\", kind=\"hnsw\"),\n",
    "                    ExhaustiveKnnAlgorithmConfiguration(name=\"my-eknn-vector-config\", kind=\"exhaustiveKnn\")\n",
    "                ],\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"insurance-profile\",\n",
    "                        algorithm_configuration_name=\"insurance-algorithm\",\n",
    "                        vectorizer_name=\"insurance-vectorizer\"\n",
    "                    )\n",
    "                ],\n",
    "                vectorizers=[vectorizer]\n",
    "            )\n",
    "            \n",
    "            # Define semantic search configuration\n",
    "            semantic_config = SemanticConfiguration(\n",
    "                name=\"insurance-semantic\",  # Fixed to match SearchTester\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"title\"),\n",
    "                    content_fields=[SemanticField(field_name=\"content\")],\n",
    "                    keywords_fields=[\n",
    "                        SemanticField(field_name=\"category\"),\n",
    "                        SemanticField(field_name=\"file_name\")\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            semantic_search = SemanticSearch(\n",
    "                configurations=[semantic_config]\n",
    "            )\n",
    "            \n",
    "            # Define the search index schema\n",
    "            fields = [\n",
    "                SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "                SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "                SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "                SearchableField(name=\"file_name\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"file_type\", type=SearchFieldDataType.String, filterable=True),\n",
    "                SimpleField(name=\"chunk_id\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_count\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"original_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"chunk_length\", type=SearchFieldDataType.Int32),\n",
    "                SimpleField(name=\"processing_date\", type=SearchFieldDataType.DateTimeOffset),\n",
    "                \n",
    "                # Vector field for integrated vectorization\n",
    "                SearchField(\n",
    "                    name=\"content_vector\",\n",
    "                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True,\n",
    "                    vector_search_dimensions=1536,  # ada-002 embedding dimension\n",
    "                    vector_search_profile_name=\"insurance-profile\"\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Create the search index\n",
    "            index = SearchIndex(\n",
    "                name=self.index_name,\n",
    "                fields=fields,\n",
    "                vector_search=vector_search,\n",
    "                semantic_search=semantic_search\n",
    "            )\n",
    "            \n",
    "            # Create or update the index\n",
    "            result = self.search_index_client.create_or_update_index(index)\n",
    "            print(f\"‚úÖ Search index '{self.index_name}' created successfully!\")\n",
    "            print(f\"üìã Index fields: {len(result.fields)}\")\n",
    "            print(f\"üîç Vector search enabled: {bool(result.vector_search)}\")\n",
    "            print(f\"üß† Semantic search enabled: {bool(result.semantic_search)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating search index: {e}\")\n",
    "            print(f\"üîç Debug info:\")\n",
    "            print(f\"   - Original endpoint: {Config.AZURE_OPENAI_ENDPOINT}\")\n",
    "            print(f\"   - Deployment name: {Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT}\")\n",
    "            print(f\"   - API key present: {bool(Config.AZURE_OPENAI_API_KEY)}\")\n",
    "            \n",
    "            # Add more detailed error information\n",
    "            import traceback\n",
    "            print(f\"üìã Full error details:\\n{traceback.format_exc()}\")\n",
    "            return False\n",
    "    \n",
    "    def delete_index_if_exists(self) -> bool:\n",
    "        \"\"\"Delete the index if it exists\"\"\"\n",
    "        try:\n",
    "            self.search_index_client.delete_index(self.index_name)\n",
    "            print(f\"‚úÖ Deleted existing index: {self.index_name}\")\n",
    "            return True\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ÑπÔ∏è Index {self.index_name} doesn't exist - will create new\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_index_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search index\"\"\"\n",
    "        try:\n",
    "            index = self.search_index_client.get_index(self.index_name)\n",
    "            stats = self.search_index_client.get_index_statistics(self.index_name)\n",
    "            \n",
    "            # Handle both object and dictionary responses\n",
    "            if hasattr(stats, 'document_count'):\n",
    "                # Object response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.document_count,\n",
    "                    \"storage_size\": stats.storage_size,\n",
    "                    \"vector_index_size\": getattr(stats, 'vector_index_size', 0)\n",
    "                }\n",
    "            else:\n",
    "                # Dictionary response\n",
    "                return {\n",
    "                    \"name\": index.name,\n",
    "                    \"field_count\": len(index.fields),\n",
    "                    \"document_count\": stats.get('document_count', 0),\n",
    "                    \"storage_size\": stats.get('storage_size', 0),\n",
    "                    \"vector_index_size\": stats.get('vector_index_size', 0)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting index stats: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialize search index manager\n",
    "if search_index_client:\n",
    "    index_manager = SearchIndexManager(search_index_client)\n",
    "    \n",
    "    # Option to recreate index (uncomment if needed)\n",
    "    # print(\"üîÑ Recreating search index...\")\n",
    "    # index_manager.delete_index_if_exists()\n",
    "    \n",
    "    success = index_manager.create_search_index()\n",
    "    if success:\n",
    "        print(\"\\nüìä Index created successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to create search index\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create search index - missing search client\")\n",
    "    index_manager = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Retrieval and Processing\n",
    "\n",
    "The next cell defines two essential classes: `DocumentRetriever` handles downloading processed documents from Azure Blob Storage, while `TextChunker` intelligently splits large documents into optimally-sized chunks with overlapping content. These components prepare the insurance documents for efficient indexing and retrieval in the search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processors initialized\n"
     ]
    }
   ],
   "source": [
    "# Reuse the DocumentRetriever class from previous notebook\n",
    "class DocumentRetriever:\n",
    "    \"\"\"Class to handle document retrieval from blob storage\"\"\"\n",
    "    \n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_all_processed_documents(self) -> Dict:\n",
    "        \"\"\"Get all processed documents ready for vectorization\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(Config.PROCESSED_CONTAINER)\n",
    "            blob_client = container_client.get_blob_client(\"processed_documents_for_vectorization.json\")\n",
    "            \n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            documents = json.loads(blob_data.decode('utf-8'))\n",
    "            \n",
    "            print(f\"‚úÖ Downloaded processed documents\")\n",
    "            return documents\n",
    "                \n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ùå File not found: processed_documents_for_vectorization.json\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading documents: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Text chunking class (simplified for search index)\n",
    "class TextChunker:\n",
    "    \"\"\"Class to handle intelligent text chunking for search index\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text_for_search(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Create chunks optimized for search index\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        chunks = []\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'chunk_id': 0,\n",
    "                'chunk_count': 1,\n",
    "                'metadata': metadata.copy()\n",
    "            }]\n",
    "        \n",
    "        # Simple sliding window chunking\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            \n",
    "            # Try to break at sentence boundaries\n",
    "            if end < len(text):\n",
    "                sentence_end = text.rfind('.', start, end)\n",
    "                if sentence_end > start:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'content': chunk_text,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_count': 0,  # Will be updated later\n",
    "                    'metadata': metadata.copy()\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = max(start + self.chunk_size - self.chunk_overlap, end)\n",
    "        \n",
    "        # Update chunk count\n",
    "        for chunk in chunks:\n",
    "            chunk['chunk_count'] = len(chunks)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize processors\n",
    "if blob_service_client:\n",
    "    retriever = DocumentRetriever(blob_service_client)\n",
    "    chunker = TextChunker(\n",
    "        chunk_size=Config.CHUNK_SIZE,\n",
    "        chunk_overlap=Config.CHUNK_OVERLAP\n",
    "    )\n",
    "    print(\"‚úÖ Document processors initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve and Process Documents\n",
    "\n",
    "The next cell implements an enhanced document retrieval system that downloads processed insurance documents from Azure Blob Storage and prepares them for search indexing with detailed error handling and debugging capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced document retriever initialized\n",
      "\n",
      "============================================================\n",
      "üì• RETRIEVING PROCESSED DOCUMENTS FROM BLOB STORAGE\n",
      "============================================================\n",
      "üîç Attempting to retrieve from container: processed-documents\n",
      "üì• Downloading file: processed_documents_for_vectorization.json\n",
      "‚úÖ File found - Size: 0.07 MB\n",
      "üì• Downloading blob content...\n",
      "üîÑ Parsing JSON content...\n",
      "‚úÖ Successfully downloaded and parsed processed documents\n",
      "üìä Found categories: ['policies', 'claims', 'statements']\n",
      "   - policies: 5/5 successful documents\n",
      "   - claims: 5/5 successful documents\n",
      "   - statements: 5/5 successful documents\n",
      "\n",
      "üéâ SUCCESS! Retrieved processed documents from blob storage\n",
      "üìä Available categories: ['policies', 'claims', 'statements']\n",
      "üéØ Filtering to process POLICIES only...\n",
      "üìÑ Found 5 policy documents\n",
      "\n",
      "üìÇ Processing policies documents...\n",
      "‚úÖ Processing 5 successful policies documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing policies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 1709.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Prepared 44 policy document chunks for search index\n",
      "\n",
      "üìä POLICIES INDEXING SUMMARY:\n",
      "   üìÑ Total policy files: 5\n",
      "   üóÇÔ∏è Total chunks created: 44\n",
      "   üìè Average chunk length: 845 characters\n",
      "\n",
      "üìã Policy files breakdown:\n",
      "   ‚Ä¢ commercial_auto_policy.md: 9 chunks\n",
      "   ‚Ä¢ comprehensive_auto_policy.md: 7 chunks\n",
      "   ‚Ä¢ high_value_vehicle_policy.md: 10 chunks\n",
      "   ‚Ä¢ liability_only_policy.md: 7 chunks\n",
      "   ‚Ä¢ motorcycle_policy.md: 11 chunks\n",
      "\n",
      "üîç Final check - search_documents variable has 44 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced document retriever with better error handling and debugging\n",
    "class EnhancedDocumentRetriever:\n",
    "    \"\"\"Enhanced class to handle document retrieval with detailed debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_all_processed_documents(self) -> Dict:\n",
    "        \"\"\"Get all processed documents with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"üîç Attempting to retrieve from container: {Config.PROCESSED_CONTAINER}\")\n",
    "            \n",
    "            # Get container client\n",
    "            container_client = self.blob_service_client.get_container_client(Config.PROCESSED_CONTAINER)\n",
    "            \n",
    "            # Try to access the specific file\n",
    "            blob_name = \"processed_documents_for_vectorization.json\"\n",
    "            print(f\"üì• Downloading file: {blob_name}\")\n",
    "            \n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "            \n",
    "            # Check if blob exists first\n",
    "            try:\n",
    "                blob_props = blob_client.get_blob_properties()\n",
    "                file_size = blob_props.size\n",
    "                print(f\"‚úÖ File found - Size: {file_size / (1024*1024):.2f} MB\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå File access error: {e}\")\n",
    "                return {}\n",
    "            \n",
    "            # Download the blob\n",
    "            print(\"üì• Downloading blob content...\")\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            \n",
    "            # Parse JSON\n",
    "            print(\"üîÑ Parsing JSON content...\")\n",
    "            documents = json.loads(blob_data.decode('utf-8'))\n",
    "            \n",
    "            print(f\"‚úÖ Successfully downloaded and parsed processed documents\")\n",
    "            print(f\"üìä Found categories: {list(documents.keys())}\")\n",
    "            \n",
    "            # Show some stats\n",
    "            for category, docs in documents.items():\n",
    "                successful_docs = [d for d in docs if d.get('success', False)]\n",
    "                print(f\"   - {category}: {len(successful_docs)}/{len(docs)} successful documents\")\n",
    "            \n",
    "            return documents\n",
    "                \n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"‚ùå File not found: {blob_name}\")\n",
    "            print(f\"   Container: {Config.PROCESSED_CONTAINER}\")\n",
    "            print(\"   This means the file doesn't exist in the specified container\")\n",
    "            return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå JSON parsing error: {e}\")\n",
    "            print(\"   The file exists but contains invalid JSON\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error downloading documents: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            print(f\"   Full traceback: {traceback.format_exc()}\")\n",
    "            return {}\n",
    "\n",
    "# Replace the original retriever and try document retrieval\n",
    "if blob_service_client:\n",
    "    retriever = EnhancedDocumentRetriever(blob_service_client)\n",
    "    print(\"‚úÖ Enhanced document retriever initialized\")\n",
    "    \n",
    "    # Now try to retrieve the documents\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì• RETRIEVING PROCESSED DOCUMENTS FROM BLOB STORAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    processed_documents = retriever.get_all_processed_documents()\n",
    "    \n",
    "    if processed_documents:\n",
    "        print(f\"\\nüéâ SUCCESS! Retrieved processed documents from blob storage\")\n",
    "        print(f\"üìä Available categories: {list(processed_documents.keys())}\")\n",
    "        \n",
    "        # Filter to only process POLICIES\n",
    "        policies_only = {'policies': processed_documents.get('policies', [])}\n",
    "        \n",
    "        print(f\"üéØ Filtering to process POLICIES only...\")\n",
    "        print(f\"üìÑ Found {len(policies_only['policies'])} policy documents\")\n",
    "        \n",
    "        # Process only policy documents into search-ready chunks\n",
    "        search_documents = []\n",
    "        \n",
    "        for category, docs in policies_only.items():\n",
    "            print(f\"\\nüìÇ Processing {category} documents...\")\n",
    "            \n",
    "            successful_docs = [doc for doc in docs if doc.get('success', False)]\n",
    "            print(f\"‚úÖ Processing {len(successful_docs)} successful {category} documents\")\n",
    "            \n",
    "            for doc in tqdm(successful_docs, desc=f\"Processing {category}\"):\n",
    "                # Get text content from policies (markdown files)\n",
    "                text_content = doc.get('text', '')\n",
    "                if not text_content:\n",
    "                    print(f\"‚ö†Ô∏è Skipping document with no text content: {doc.get('metadata', {}).get('file_name', 'Unknown')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare metadata\n",
    "                metadata = doc.get('metadata', {}).copy()\n",
    "                metadata['category'] = category\n",
    "                \n",
    "                # Create chunks for this document\n",
    "                chunks = chunker.chunk_text_for_search(text_content, metadata)\n",
    "                \n",
    "                # Convert chunks to search documents\n",
    "                for chunk in chunks:\n",
    "                    search_doc = {\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'title': f\"{metadata.get('file_name', 'Unknown')} - Part {chunk['chunk_id'] + 1}\",\n",
    "                        'content': chunk['content'],\n",
    "                        'category': category,\n",
    "                        'file_name': metadata.get('file_name', 'Unknown'),\n",
    "                        'file_type': metadata.get('file_type', 'markdown'),\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'chunk_count': chunk['chunk_count'],\n",
    "                        'original_length': len(text_content),\n",
    "                        'chunk_length': len(chunk['content']),\n",
    "                        'processing_date': datetime.now().isoformat() + 'Z'\n",
    "                    }\n",
    "                    search_documents.append(search_doc)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Prepared {len(search_documents)} policy document chunks for search index\")\n",
    "        \n",
    "        # Show detailed statistics for policies only\n",
    "        if search_documents:\n",
    "            total_files = len(set(doc['file_name'] for doc in search_documents))\n",
    "            total_chunks = len(search_documents)\n",
    "            avg_chunk_length = sum(doc['chunk_length'] for doc in search_documents) / total_chunks\n",
    "            \n",
    "            print(f\"\\nüìä POLICIES INDEXING SUMMARY:\")\n",
    "            print(f\"   üìÑ Total policy files: {total_files}\")\n",
    "            print(f\"   üóÇÔ∏è Total chunks created: {total_chunks}\")\n",
    "            print(f\"   üìè Average chunk length: {avg_chunk_length:.0f} characters\")\n",
    "            \n",
    "            # Show file breakdown\n",
    "            file_stats = {}\n",
    "            for doc in search_documents:\n",
    "                file_name = doc['file_name']\n",
    "                if file_name not in file_stats:\n",
    "                    file_stats[file_name] = 0\n",
    "                file_stats[file_name] += 1\n",
    "            \n",
    "            print(f\"\\nüìã Policy files breakdown:\")\n",
    "            for file_name, chunk_count in file_stats.items():\n",
    "                print(f\"   ‚Ä¢ {file_name}: {chunk_count} chunks\")\n",
    "        else:\n",
    "            print(\"‚ùå No policy documents were processed successfully\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Still unable to retrieve documents from blob storage\")\n",
    "        print(\"üí° Troubleshooting steps:\")\n",
    "        print(\"   1. Verify the file exists in blob storage using Azure Portal\") \n",
    "        print(\"   2. Check that the container name 'processed-documents' is correct\")\n",
    "        print(\"   3. Ensure your storage connection string has the right permissions\")\n",
    "        print(\"   4. Try running the document processing notebook (1.document-processing.ipynb) first\")\n",
    "        search_documents = []\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed - blob service client not available\")\n",
    "    search_documents = []\n",
    "\n",
    "print(f\"\\nüîç Final check - search_documents variable has {len(search_documents) if 'search_documents' in locals() else 0} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Documents to Azure AI Search with Integrated Vectorization\n",
    "\n",
    "The next cell implements a `SearchIndexUploader` class that efficiently uploads the processed policy document chunks to Azure AI Search in batches, with automatic embedding generation through integrated vectorization and comprehensive error handling and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting POLICIES upload to Azure AI Search...\n",
      "============================================================\n",
      "üéØ Uploading POLICY documents only\n",
      "‚ÑπÔ∏è Azure AI Search will automatically generate embeddings using integrated vectorization\n",
      "üì§ Uploading 44 documents to search index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document upload completed!\n",
      "\n",
      "‚è≥ Waiting for indexing to complete...\n",
      "‚úÖ Index now contains 44 policy document chunks\n",
      "üìä Index statistics:\n",
      "   - Policy documents: 0\n",
      "   - Storage size: 0 bytes\n",
      "   - Vector index size: 0 bytes\n",
      "\n",
      "üéØ SUCCESS: Only policy documents have been indexed!\n",
      "üìÑ Your Azure AI Search index now contains comprehensive policy information\n",
      "üîç Ready for policy-related queries and AI agent integration\n"
     ]
    }
   ],
   "source": [
    "class SearchIndexUploader:\n",
    "    \"\"\"Class to upload documents to Azure AI Search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def upload_documents_batch(self, documents: List[Dict], batch_size: int = 50) -> bool:\n",
    "        \"\"\"Upload documents to search index in batches\"\"\"\n",
    "        try:\n",
    "            total_docs = len(documents)\n",
    "            print(f\"üì§ Uploading {total_docs} documents to search index...\")\n",
    "            \n",
    "            # Upload in batches\n",
    "            for i in tqdm(range(0, total_docs, batch_size), desc=\"Uploading batches\"):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                \n",
    "                # Prepare batch for upload (Azure AI Search will handle vectorization)\n",
    "                upload_batch = []\n",
    "                for doc in batch:\n",
    "                    # Remove any fields that shouldn't be in the search document\n",
    "                    search_doc = doc.copy()\n",
    "                    upload_batch.append(search_doc)\n",
    "                \n",
    "                # Upload batch\n",
    "                result = self.search_client.upload_documents(documents=upload_batch)\n",
    "                \n",
    "                # Check for errors\n",
    "                failed_docs = [r for r in result if not r.succeeded]\n",
    "                if failed_docs:\n",
    "                    print(f\"‚ö†Ô∏è Failed to upload {len(failed_docs)} documents in batch {i//batch_size + 1}\")\n",
    "                    for failed in failed_docs[:3]:  # Show first 3 errors\n",
    "                        print(f\"   Error: {failed.error_message}\")\n",
    "            \n",
    "            print(f\"‚úÖ Document upload completed!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get the current document count in the index\"\"\"\n",
    "        try:\n",
    "            # Simple search to get document count\n",
    "            results = self.search_client.search(\"*\", include_total_count=True, top=1)\n",
    "            return results.get_count()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting document count: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Upload documents to search index\n",
    "if search_client and search_documents:\n",
    "    uploader = SearchIndexUploader(search_client)\n",
    "    \n",
    "    print(\"\\nüöÄ Starting POLICIES upload to Azure AI Search...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ Uploading POLICY documents only\")\n",
    "    print(\"‚ÑπÔ∏è Azure AI Search will automatically generate embeddings using integrated vectorization\")\n",
    "    \n",
    "    success = uploader.upload_documents_batch(search_documents)\n",
    "    \n",
    "    if success:\n",
    "        # Wait a moment for indexing to complete\n",
    "        import time\n",
    "        print(\"\\n‚è≥ Waiting for indexing to complete...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Get final document count\n",
    "        doc_count = uploader.get_document_count()\n",
    "        print(f\"‚úÖ Index now contains {doc_count} policy document chunks\")\n",
    "        \n",
    "        # Get index statistics\n",
    "        if index_manager:\n",
    "            stats = index_manager.get_index_stats()\n",
    "            if stats:\n",
    "                print(f\"üìä Index statistics:\")\n",
    "                print(f\"   - Policy documents: {stats.get('document_count', 'N/A')}\")\n",
    "                print(f\"   - Storage size: {stats.get('storage_size', 'N/A')} bytes\")\n",
    "                print(f\"   - Vector index size: {stats.get('vector_index_size', 'N/A')} bytes\")\n",
    "        \n",
    "        print(f\"\\nüéØ SUCCESS: Only policy documents have been indexed!\")\n",
    "        print(f\"üìÑ Your Azure AI Search index now contains comprehensive policy information\")\n",
    "        print(f\"üîç Ready for policy-related queries and AI agent integration\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to upload policy documents to search index\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot upload documents - missing search client or policy documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Search Index with Semantic and Vector Search\n",
    "\n",
    "The next cell defines a `SearchTester` class that provides comprehensive testing capabilities for the Azure AI Search index, including semantic search with reranking, hybrid search combining keyword and vector approaches, and formatted result display with relevance scores and content previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Search tester initialized\n"
     ]
    }
   ],
   "source": [
    "class SearchTester:\n",
    "    \"\"\"Class to test the search index with various query types\"\"\"\n",
    "    \n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self.search_client = search_client\n",
    "    \n",
    "    def vector_search(self, query: str, top_k: int = 5, category_filter: str = None) -> List[Dict]:\n",
    "        \"\"\"Perform vector search using integrated vectorization\"\"\"\n",
    "        try:\n",
    "            # Build search parameters\n",
    "            search_params = {\n",
    "                \"search_text\": query,\n",
    "                \"top\": top_k,\n",
    "                \"search_mode\": \"any\",\n",
    "                \"query_type\": \"semantic\",\n",
    "                \"semantic_configuration_name\": \"insurance-semantic\",\n",
    "                \"select\": [\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\", \"chunk_count\"]\n",
    "            }\n",
    "            \n",
    "            # Add category filter if specified\n",
    "            if category_filter:\n",
    "                search_params[\"filter\"] = f\"category eq '{category_filter}'\"\n",
    "            \n",
    "            # Perform search\n",
    "            results = self.search_client.search(**search_params)\n",
    "            \n",
    "            # Convert results to list\n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'chunk_count': result['chunk_count'],\n",
    "                    'score': result.get('@search.score', 0),\n",
    "                    'reranker_score': result.get('@search.reranker_score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in vector search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search (keyword + vector)\"\"\"\n",
    "        try:\n",
    "            results = self.search_client.search(\n",
    "                search_text=query,\n",
    "                top=top_k,\n",
    "                search_mode=\"all\",\n",
    "                include_total_count=True,\n",
    "                select=[\"id\", \"title\", \"content\", \"category\", \"file_name\", \"chunk_id\"]\n",
    "            )\n",
    "            \n",
    "            search_results = []\n",
    "            for result in results:\n",
    "                search_results.append({\n",
    "                    'id': result['id'],\n",
    "                    'title': result['title'],\n",
    "                    'content': result['content'],\n",
    "                    'category': result['category'],\n",
    "                    'file_name': result['file_name'],\n",
    "                    'chunk_id': result['chunk_id'],\n",
    "                    'score': result.get('@search.score', 0)\n",
    "                })\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in hybrid search: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def display_search_results(self, query: str, results: List[Dict], search_type: str = \"Search\"):\n",
    "        \"\"\"Display search results in a formatted way\"\"\"\n",
    "        print(f\"\\nüîç {search_type} Results for: '{query}'\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results found.\")\n",
    "            return\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            score = result.get('score', 0)\n",
    "            reranker_score = result.get('reranker_score', 0)\n",
    "            \n",
    "            print(f\"\\n{i}. üìÑ {result['title']}\")\n",
    "            print(f\"   üìÇ Category: {result['category']}\")\n",
    "            print(f\"   üìä Score: {score:.4f}\", end=\"\")\n",
    "            if reranker_score > 0:\n",
    "                print(f\" | Reranker: {reranker_score:.4f}\")\n",
    "            else:\n",
    "                print()\n",
    "            print(f\"   üìù Chunk {result['chunk_id'] + 1}\")\n",
    "            \n",
    "            # Show preview of content\n",
    "            preview = result['content'][:300]\n",
    "            if len(result['content']) > 300:\n",
    "                preview += \"...\"\n",
    "            print(f\"   üí¨ Preview: {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Initialize search tester\n",
    "if search_client:\n",
    "    search_tester = SearchTester(search_client)\n",
    "    print(\"‚úÖ Search tester initialized\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize search tester - missing search client\")\n",
    "    search_tester = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with Sample Insurance Queries\n",
    "\n",
    "The next cell executes a comprehensive test suite using predefined insurance-related queries to validate the search index functionality, demonstrating semantic search capabilities across various insurance topics like collision coverage, liability limits, and policy requirements with detailed result analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Azure AI Search with integrated vectorization...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üîç Testing query: 'What is covered under collision insurance?'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'What is covered under collision insurance?'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ comprehensive_auto_policy.md - Part 1\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.2905 | Reranker: 2.7784\n",
      "   üìù Chunk 1\n",
      "   üí¨ Preview: # Comprehensive Auto Insurance Policy **Policy Type:** Comprehensive Auto Insurance **Coverage Category:** Full Coverage **Policy Code:** COMP-AUTO-001 ## Section 1: Coverage Overview This comprehensive auto insurance policy provides extensive protection for your vehicle and liability coverage for d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ motorcycle_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.6658 | Reranker: 2.6203\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ comprehensive_auto_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 0.6448 | Reranker: 2.6012\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'How much does comprehensive coverage cost?'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'How much does comprehensive coverage cost?'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ comprehensive_auto_policy.md - Part 3\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.5431 | Reranker: 2.7387\n",
      "   üìù Chunk 3\n",
      "   üí¨ Preview: 2: Premium Limits (Available) - Collision: Up to $100,000 per incident - Comprehensive: Up to $100,000 per incident - Bodily Injury Liability: Up to $500,000 per person, $1,000,000 per accident - Property Damage Liability: Up to $500,000 per accident - Medical Payments: Up to $50,000 per person ## S...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ high_value_vehicle_policy.md - Part 5\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 0.9922 | Reranker: 2.7224\n",
      "   üìù Chunk 5\n",
      "   üí¨ Preview: ctible Options - Comprehensive: $0, $1,000, $2,500, $5,000 - Collision: $0, $1,000, $2,500, $5,000, $10,000 - Disappearing deductible programs available - Separate deductibles for different coverage types ## Specialized Endorsements ### Track Day and Racing Coverage - Organized track events and driv...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ comprehensive_auto_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.2455 | Reranker: 2.7167\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'What are the liability limits for commercial vehicles?'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'What are the liability limits for commercial vehicles?'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ commercial_auto_policy.md - Part 4\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 3.5609 | Reranker: 2.9351\n",
      "   üìù Chunk 4\n",
      "   üí¨ Preview: protection while in care, custody, and control - Garage keepers legal liability - False pretense coverage ## Coverage Limits ### Standard Commercial Limits - Bodily Injury Liability: $500,000 per person, $1,000,000 per accident - Property Damage Liability: $500,000 per accident - Medical Payments: $...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ commercial_auto_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 5.7855 | Reranker: 2.6061\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: - Service and repair operations - Sales and delivery services - Construction and contracting - Transportation and logistics - Professional services (real estate, consulting) - Retail and wholesale operations ## Coverage Components ### Liability Coverage - Bodily injury to third parties during busine...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ high_value_vehicle_policy.md - Part 4\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 3.5490 | Reranker: 2.3725\n",
      "   üìù Chunk 4\n",
      "   üí¨ Preview: Appraisal and Valuation Services - Annual professional appraisals - Market value tracking and updates - Documentation and photography services - Condition reports and maintenance records - Authenticity verification for classics ### Risk Management Services - Security system recommendations - Storage...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'Does my policy cover theft and vandalism?'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'Does my policy cover theft and vandalism?'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ comprehensive_auto_policy.md - Part 1\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.2224 | Reranker: 2.8656\n",
      "   üìù Chunk 1\n",
      "   üí¨ Preview: # Comprehensive Auto Insurance Policy **Policy Type:** Comprehensive Auto Insurance **Coverage Category:** Full Coverage **Policy Code:** COMP-AUTO-001 ## Section 1: Coverage Overview This comprehensive auto insurance policy provides extensive protection for your vehicle and liability coverage for d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ motorcycle_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.1842 | Reranker: 2.7621\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ high_value_vehicle_policy.md - Part 3\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.2033 | Reranker: 2.7415\n",
      "   üìù Chunk 3\n",
      "   üí¨ Preview: ing and finish quality guarantees ### Enhanced Comprehensive Coverage - Theft protection with GPS tracking requirements - Vandalism and malicious mischief coverage - Weather damage protection (hail, flood, wind) - Fire and explosion coverage - Falling object protection - Animal collision coverage wi...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'What happens if I hit an uninsured driver?'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'What happens if I hit an uninsured driver?'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ comprehensive_auto_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 4.3855 | Reranker: 2.8287\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 3: Liability Coverage - Bodily injury to third parties - Property damage to other vehicles or structures - Legal defense costs - Court-ordered judgments and settlements ### Section 2.4: Medical Payments Coverage - Medical expenses for driver and passengers - Hospital and emergency room costs - Rehab...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ motorcycle_policy.md - Part 3\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 3.8223 | Reranker: 2.5877\n",
      "   üìù Chunk 3\n",
      "   üí¨ Preview: 3: Medical Payments Coverage - Medical expenses for rider and passenger - Emergency room and hospital costs - Ambulance transportation - Rehabilitation and physical therapy - Dental and vision care related to accidents ### Section 3.4: Uninsured/Underinsured Motorist Coverage - Protection against un...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ commercial_auto_policy.md - Part 3\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.2937 | Reranker: 2.3796\n",
      "   üìù Chunk 3\n",
      "   üí¨ Preview: t ### Medical Payments Coverage - Medical expenses for employees injured in business vehicles - Coverage for business guests and passengers - Emergency medical treatment costs - Ambulance and hospital expenses ### Uninsured Motorist Coverage - Protection against uninsured drivers during business ope...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'High value vehicle insurance requirements'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'High value vehicle insurance requirements'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ high_value_vehicle_policy.md - Part 4\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 5.1741 | Reranker: 2.9252\n",
      "   üìù Chunk 4\n",
      "   üí¨ Preview: Appraisal and Valuation Services - Annual professional appraisals - Market value tracking and updates - Documentation and photography services - Condition reports and maintenance records - Authenticity verification for classics ### Risk Management Services - Security system recommendations - Storage...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ high_value_vehicle_policy.md - Part 1\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 5.7115 | Reranker: 2.8564\n",
      "   üìù Chunk 1\n",
      "   üí¨ Preview: # High-Value Vehicle Insurance Policy **Policy Type:** High-Value Vehicle Insurance **Coverage Category:** Luxury and Exotic Vehicle Coverage **Policy Code:** HV-AUTO-001 ## Coverage Overview This specialized high-value vehicle insurance policy provides comprehensive protection for luxury, exotic, c...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ high_value_vehicle_policy.md - Part 6\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.8483 | Reranker: 2.8415\n",
      "   üìù Chunk 6\n",
      "   üí¨ Preview: arts - Tools and equipment coverage - Memorabilia and documentation ## Underwriting Requirements ### Vehicle Documentation - Professional appraisal (required for vehicles over $100,000) - Detailed photographs and condition reports - Maintenance records and service history - Modification documentatio...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üîç Testing query: 'Motorcycle insurance coverage options'\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "üîç Semantic Search Results for: 'Motorcycle insurance coverage options'\n",
      "================================================================================\n",
      "\n",
      "1. üìÑ motorcycle_policy.md - Part 3\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.4857 | Reranker: 3.2151\n",
      "   üìù Chunk 3\n",
      "   üí¨ Preview: 3: Medical Payments Coverage - Medical expenses for rider and passenger - Emergency room and hospital costs - Ambulance transportation - Rehabilitation and physical therapy - Dental and vision care related to accidents ### Section 3.4: Uninsured/Underinsured Motorist Coverage - Protection against un...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. üìÑ motorcycle_policy.md - Part 2\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 1.4933 | Reranker: 3.1506\n",
      "   üìù Chunk 2\n",
      "   üí¨ Preview: 50cc-250cc) - Medium displacement (251cc-600cc) - Large displacement (601cc-1000cc) - High-performance (1000cc+) - Electric motorcycles (by power rating) ## Section 3: Coverage Components ### Section 3.1: Liability Coverage - Bodily injury to other parties - Property damage to other vehicles and pro...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. üìÑ motorcycle_policy.md - Part 6\n",
      "   üìÇ Category: policies\n",
      "   üìä Score: 2.6373 | Reranker: 3.0648\n",
      "   üìù Chunk 6\n",
      "   üí¨ Preview: rage - Extended trip protection - Out-of-state coverage enhancement - International travel (Canada/Mexico) - Camping and touring equipment - Emergency transportation home ### Antique and Classic Motorcycle Coverage - Agreed value coverage for classics - Show and exhibition coverage - Restoration cov...\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "‚úÖ Query testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the search index with sample queries\n",
    "if search_tester:\n",
    "    test_queries = [\n",
    "        \"What is covered under collision insurance?\",\n",
    "        \"How much does comprehensive coverage cost?\", \n",
    "        \"What are the liability limits for commercial vehicles?\",\n",
    "        \"Does my policy cover theft and vandalism?\",\n",
    "        \"What happens if I hit an uninsured driver?\",\n",
    "        \"High value vehicle insurance requirements\",\n",
    "        \"Motorcycle insurance coverage options\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing Azure AI Search with integrated vectorization...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\nüîç Testing query: '{query}'\")\n",
    "        \n",
    "        # Test semantic search\n",
    "        results = search_tester.vector_search(query, top_k=3)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"‚úÖ Found {len(results)} relevant chunks\")\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "        else:\n",
    "            print(\"‚ùå No relevant documents found\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n‚úÖ Query testing completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot test queries - search tester not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Search Interface\n",
    "\n",
    "The next cell provides an interactive search function that creates a user-friendly command-line interface for real-time testing of the Azure AI Search index, allowing users to enter natural language queries, apply category filters, and compare semantic versus hybrid search results interactively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Interactive Azure AI Search Interface\n",
      "==================================================\n",
      "Enter your search queries (type 'quit' to exit)\n",
      "Optional commands:\n",
      "  - Add 'category:policies' or 'category:claims' to filter results\n",
      "  - Use natural language queries for best semantic search results\n",
      "\n",
      "\n",
      "üß† Performing semantic search...\n",
      "\n",
      "üîç Semantic Search Results for: 'Broken wheels'\n",
      "================================================================================\n",
      "No results found.\n",
      "\n",
      "üîÑ Hybrid search results:\n",
      "\n",
      "üëã Search session ended\n"
     ]
    }
   ],
   "source": [
    "def interactive_search():\n",
    "    \"\"\"Interactive search interface for testing\"\"\"\n",
    "    if not search_tester:\n",
    "        print(\"‚ùå Search tester not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîç Interactive Azure AI Search Interface\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Enter your search queries (type 'quit' to exit)\")\n",
    "    print(\"Optional commands:\")\n",
    "    print(\"  - Add 'category:policies' or 'category:claims' to filter results\")\n",
    "    print(\"  - Use natural language queries for best semantic search results\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nüîç Search: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # Check for category filter\n",
    "            category_filter = None\n",
    "            if 'category:' in query:\n",
    "                parts = query.split('category:')\n",
    "                query = parts[0].strip()\n",
    "                category_filter = parts[1].strip()\n",
    "            \n",
    "            # Perform semantic search\n",
    "            print(f\"\\nüß† Performing semantic search...\")\n",
    "            results = search_tester.vector_search(query, top_k=5, category_filter=category_filter)\n",
    "            \n",
    "            # Display results\n",
    "            search_tester.display_search_results(query, results, \"Semantic Search\")\n",
    "            \n",
    "            # Also try hybrid search for comparison\n",
    "            print(f\"\\nüîÑ Hybrid search results:\")\n",
    "            hybrid_results = search_tester.hybrid_search(query, top_k=3)\n",
    "            if hybrid_results:\n",
    "                for i, result in enumerate(hybrid_results[:2], 1):  # Show top 2\n",
    "                    print(f\"{i}. {result['title']} (Score: {result['score']:.4f})\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\nüëã Search session ended\")\n",
    "\n",
    "# Note: Uncomment the line below to start interactive search\n",
    "interactive_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "The next cell generates a comprehensive summary of the entire Azure AI Search integration process, collecting index statistics, document counts, and search capabilities to provide a detailed final report of what was accomplished and confirm the system's readiness for AI agent integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã AZURE AI SEARCH INTEGRATION SUMMARY\n",
      "============================================================\n",
      "üìÖ Completed: 2025-09-25T18:47:40.057649\n",
      "üîç Search Service: msagthack-search-m3qz57ik6ngog\n",
      "üìá Search Index: insurance-documents-index\n",
      "üìÑ Documents Processed: 10\n",
      "üóÇÔ∏è Chunks Indexed: 44\n",
      "ü§ñ Embedding Model: text-embedding-ada-002\n",
      "‚ö° Vectorization: Azure AI Search Integrated Vectorization\n",
      "\n",
      "üöÄ SEARCH CAPABILITIES:\n",
      "  ‚úÖ Semantic Search\n",
      "  ‚úÖ Vector Search\n",
      "  ‚úÖ Hybrid Search\n",
      "  ‚úÖ Automatic Vectorization\n",
      "  ‚úÖ Real Time Indexing\n",
      "\n",
      "üìä BY CATEGORY:\n",
      "  ‚Ä¢ Policies:\n",
      "    - Files: 5\n",
      "    - Chunks: 44\n",
      "    - Total Characters: 37,186\n"
     ]
    }
   ],
   "source": [
    "# Generate final summary\n",
    "def generate_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the Azure AI Search integration\"\"\"\n",
    "    \n",
    "    # Get index statistics\n",
    "    index_stats = {}\n",
    "    doc_count = 0\n",
    "    \n",
    "    if search_client and index_manager:\n",
    "        try:\n",
    "            doc_count = SearchIndexUploader(search_client).get_document_count()\n",
    "            index_stats = index_manager.get_index_stats()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    summary = {\n",
    "        \"integration_summary\": {\n",
    "            \"completion_date\": datetime.now().isoformat(),\n",
    "            \"search_service\": Config.SEARCH_SERVICE_NAME,\n",
    "            \"search_index\": Config.SEARCH_INDEX_NAME,\n",
    "            \"total_documents_processed\": len(processed_documents.get('policies', []) + processed_documents.get('claims', [])) if 'processed_documents' in globals() and processed_documents else 0,\n",
    "            \"total_chunks_indexed\": doc_count,\n",
    "            \"embedding_model\": Config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,\n",
    "            \"vectorization_method\": \"Azure AI Search Integrated Vectorization\",\n",
    "            \"chunk_configuration\": {\n",
    "                \"chunk_size\": Config.CHUNK_SIZE,\n",
    "                \"chunk_overlap\": Config.CHUNK_OVERLAP\n",
    "            }\n",
    "        },\n",
    "        \"search_capabilities\": {\n",
    "            \"semantic_search\": True,\n",
    "            \"vector_search\": True,\n",
    "            \"hybrid_search\": True,\n",
    "            \"automatic_vectorization\": True,\n",
    "            \"real_time_indexing\": True\n",
    "        },\n",
    "        \"index_statistics\": index_stats,\n",
    "        \"ready_for_ai_agents\": bool(doc_count > 0)\n",
    "    }\n",
    "    \n",
    "    # Add category breakdown if available\n",
    "    if 'search_documents' in globals() and search_documents:\n",
    "        category_stats = {}\n",
    "        for doc in search_documents:\n",
    "            category = doc['category']\n",
    "            if category not in category_stats:\n",
    "                category_stats[category] = {\n",
    "                    'chunks': 0,\n",
    "                    'total_characters': 0,\n",
    "                    'files': set()\n",
    "                }\n",
    "            \n",
    "            category_stats[category]['chunks'] += 1\n",
    "            category_stats[category]['total_characters'] += doc['chunk_length']\n",
    "            category_stats[category]['files'].add(doc['file_name'])\n",
    "        \n",
    "        # Convert sets to lists and add unique file counts\n",
    "        for category in category_stats:\n",
    "            category_stats[category]['files'] = list(category_stats[category]['files'])\n",
    "            category_stats[category]['unique_files'] = len(category_stats[category]['files'])\n",
    "        \n",
    "        summary['categories_processed'] = category_stats\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display summary\n",
    "final_summary = generate_summary()\n",
    "\n",
    "print(\"üìã AZURE AI SEARCH INTEGRATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÖ Completed: {final_summary['integration_summary']['completion_date']}\")\n",
    "print(f\"üîç Search Service: {final_summary['integration_summary']['search_service']}\")\n",
    "print(f\"üìá Search Index: {final_summary['integration_summary']['search_index']}\")\n",
    "print(f\"üìÑ Documents Processed: {final_summary['integration_summary']['total_documents_processed']}\")\n",
    "print(f\"üóÇÔ∏è Chunks Indexed: {final_summary['integration_summary']['total_chunks_indexed']}\")\n",
    "print(f\"ü§ñ Embedding Model: {final_summary['integration_summary']['embedding_model']}\")\n",
    "print(f\"‚ö° Vectorization: {final_summary['integration_summary']['vectorization_method']}\")\n",
    "\n",
    "print(\"\\nüöÄ SEARCH CAPABILITIES:\")\n",
    "capabilities = final_summary['search_capabilities']\n",
    "for capability, enabled in capabilities.items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {capability.replace('_', ' ').title()}\")\n",
    "\n",
    "if 'categories_processed' in final_summary:\n",
    "    print(\"\\nüìä BY CATEGORY:\")\n",
    "    for category, stats in final_summary['categories_processed'].items():\n",
    "        print(f\"  ‚Ä¢ {category.title()}:\")\n",
    "        print(f\"    - Files: {stats['unique_files']}\")\n",
    "        print(f\"    - Chunks: {stats['chunks']}\")\n",
    "        print(f\"    - Total Characters: {stats['total_characters']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
