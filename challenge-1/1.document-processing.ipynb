{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing with Azure OpenAI Multimodal Model\n",
    "\n",
    "This notebook demonstrates how to process insurance documents using Azure OpenAI's multimodal GPT model, including both text and image analysis. The workflow includes:\n",
    "\n",
    "1. **Upload Data to Azure Blob Storage**: Upload insurance policy documents (.md files), claim statements (.md files), and crash images (.jpg/.png files) to Azure Blob Storage containers for organized processing.\n",
    "\n",
    "2. **Process Documents with Azure OpenAI GPT-4-1-mini**\n",
    "   - **Text Processing**: Prepare markdown files containing insurance policies and claim statements for vectorization\n",
    "   - **Image Analysis**: Use GPT's vision capabilities to generate detailed descriptions of crash scene images, analyzing vehicle damage, environmental conditions, and relevant details for insurance claim processing\n",
    "\n",
    "3. **Extract Structured Information**: Utilize Azure OpenAI's structured output capabilities to extract key information from claim statements into structured JSON format, including policyholder details, incident information, vehicle data, and witness information.\n",
    "\n",
    "4. **Store in Azure Cosmos DB**: Save the processed and structured claim information to Azure Cosmos DB for easy retrieval and analysis by insurance agents and automated systems.\n",
    "\n",
    "5. **Retrieve Information in JSON Format**: Generate comprehensive JSON outputs containing both structured claim data and detailed image descriptions, ready for downstream processing like vectorization and RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "Automating document processing is crucial for improving efficiency and accuracy in handling large volumes of data. By leveraging Azure's cloud services, organizations can streamline their workflows, reduce manual errors, and gain valuable insights from their documents. This approach not only saves time and resources but also enhances data accessibility and decision-making capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Let's start with handling the import of our libraries and load the `.env` variables that we have saved in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Azure SDK imports\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.aio import AIProjectClient as AsyncAIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.agents.models import MessageRole, ListSortOrder, AzureAISearchTool, AzureAISearchQueryType\n",
    "# OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell initializes the Azure clients used throughout the notebook: a `BlobServiceClient` (from `AZURE_STORAGE_CONNECTION_STRING`) for uploading, listing, and downloading files in Blob Storage, and an `AzureOpenAI` client (from `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_KEY`) for text and multimodal GPT processing. The `initialize_clients()` function builds both clients, prints connection diagnostics on success, and returns `(blob_service_client, openai_client)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "üìÅ Policies directory: data/policies\n",
      "üìÅ Statements directory: data/statements\n",
      "üìÅ Claims directory: data/claims\n",
      "ü§ñ OpenAI Deployment: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Storage configuration\n",
    "    AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "    AZURE_STORAGE_ACCOUNT_NAME = os.getenv('AZURE_STORAGE_ACCOUNT_NAME')\n",
    "    AZURE_STORAGE_ACCOUNT_KEY = os.getenv('AZURE_STORAGE_ACCOUNT_KEY')\n",
    "    \n",
    "    # Azure OpenAI configuration\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "    AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1-mini')\n",
    "        \n",
    "    # Cosmos DB configuration\n",
    "    COSMOS_ENDPOINT = os.getenv('COSMOS_ENDPOINT')\n",
    "    COSMOS_KEY = os.getenv('COSMOS_KEY')\n",
    "    COSMOS_DATABASE = 'insurance_claims'\n",
    "    COSMOS_CONTAINER = 'crash_reports'\n",
    "    \n",
    "    # Container names\n",
    "    POLICIES_CONTAINER = 'policies'\n",
    "    CLAIMS_CONTAINER = 'claims'\n",
    "    PROCESSED_CONTAINER = 'processed-documents'\n",
    "    STATEMENTS_CONTAINER = 'statements'\n",
    "    \n",
    "    # Local data paths\n",
    "    DATA_DIR = Path('data')\n",
    "    POLICIES_DIR = DATA_DIR / 'policies'\n",
    "    CLAIMS_DIR = DATA_DIR / 'claims'\n",
    "    STATEMENTS_DIR = DATA_DIR / 'statements'\n",
    "\n",
    "# Validate configuration\n",
    "required_vars = [\n",
    "    Config.AZURE_STORAGE_CONNECTION_STRING,\n",
    "    Config.AZURE_OPENAI_ENDPOINT,\n",
    "    Config.AZURE_OPENAI_API_KEY\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not var]\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing environment variables. Please check your .env file.\")\n",
    "    print(\"Missing variables - please add these to your .env file:\")\n",
    "    if not Config.AZURE_OPENAI_ENDPOINT:\n",
    "        print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    if not Config.AZURE_OPENAI_API_KEY:\n",
    "        print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    if not Config.AZURE_STORAGE_CONNECTION_STRING:\n",
    "        print(\"  - AZURE_STORAGE_CONNECTION_STRING\")\n",
    "else:\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "    print(f\"üìÅ Policies directory: {Config.POLICIES_DIR}\")\n",
    "    print(f\"üìÅ Statements directory: {Config.STATEMENTS_DIR}\")\n",
    "    print(f\"üìÅ Claims directory: {Config.CLAIMS_DIR}\")\n",
    "    print(f\"ü§ñ OpenAI Deployment: {Config.AZURE_OPENAI_DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Azure Services Setup\n",
    "\n",
    "The next cell initializes Azure service clients by creating a BlobServiceClient for Azure Storage and an AzureOpenAI client for GPT-4.1-mini processing, with error handling to ensure both connections are established successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure clients initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure clients\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize Azure service clients\"\"\"\n",
    "    try:\n",
    "        # Blob Storage client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            Config.AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        \n",
    "        # Azure OpenAI client\n",
    "        openai_client = AzureOpenAI(\n",
    "            azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "            api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "            api_version=Config.AZURE_OPENAI_API_VERSION\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Azure clients initialized successfully!\")\n",
    "        return blob_service_client, openai_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing clients: {e}\")\n",
    "        return None, None\n",
    "\n",
    "blob_service_client, openai_client = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates and tests Azure Blob Storage containers with enhanced error handling, checking connections, listing existing containers, and attempting to create the required containers (policies, claims, statements, processed-documents) while providing detailed diagnostics for any failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running enhanced container creation...\n",
      "üîç Testing storage account connection...\n",
      "‚úÖ Connected to storage account successfully\n",
      "   Account kind: StorageV2\n",
      "   SKU name: Standard_LRS\n",
      "\n",
      "üîç Checking existing containers...\n",
      "‚úÖ Found 4 existing containers: ['claims', 'policies', 'processed-documents', 'statements']\n",
      "‚ÑπÔ∏è Container 'policies' already exists\n",
      "‚ÑπÔ∏è Container 'claims' already exists\n",
      "‚ÑπÔ∏è Container 'statements' already exists\n",
      "‚ÑπÔ∏è Container 'processed-documents' already exists\n",
      "\n",
      "üìä Container Creation Summary:\n",
      "   Successful: 4 - ['policies', 'claims', 'statements', 'processed-documents']\n",
      "   Failed: 0 - []\n"
     ]
    }
   ],
   "source": [
    "# Enhanced container creation with multiple authentication methods and diagnostics\n",
    "def create_containers_enhanced(blob_service_client):\n",
    "    \"\"\"Create blob storage containers with enhanced error handling and diagnostics\"\"\"\n",
    "    \n",
    "    # First, test the connection\n",
    "    try:\n",
    "        print(\"üîç Testing storage account connection...\")\n",
    "        account_info = blob_service_client.get_account_information()\n",
    "        print(f\"‚úÖ Connected to storage account successfully\")\n",
    "        print(f\"   Account kind: {account_info.get('account_kind', 'Unknown')}\")\n",
    "        print(f\"   SKU name: {account_info.get('sku_name', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to storage account: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test listing existing containers\n",
    "    try:\n",
    "        print(\"\\nüîç Checking existing containers...\")\n",
    "        existing_containers = []\n",
    "        for container in blob_service_client.list_containers():\n",
    "            existing_containers.append(container.name)\n",
    "        print(f\"‚úÖ Found {len(existing_containers)} existing containers: {existing_containers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to list containers: {e}\")\n",
    "        print(\"   This might indicate insufficient permissions\")\n",
    "    \n",
    "    # Try to create containers\n",
    "    containers = [\n",
    "        Config.POLICIES_CONTAINER,\n",
    "        Config.CLAIMS_CONTAINER,\n",
    "        Config.STATEMENTS_CONTAINER,  # Added statements container\n",
    "        Config.PROCESSED_CONTAINER\n",
    "    ]\n",
    "    \n",
    "    created_containers = []\n",
    "    failed_containers = []\n",
    "    \n",
    "    for container_name in containers:\n",
    "        try:\n",
    "            # Check if container already exists first\n",
    "            container_client = blob_service_client.get_container_client(container_name)\n",
    "            \n",
    "            try:\n",
    "                # Try to get container properties (this will fail if it doesn't exist)\n",
    "                properties = container_client.get_container_properties()\n",
    "                print(f\"‚ÑπÔ∏è Container '{container_name}' already exists\")\n",
    "                created_containers.append(container_name)\n",
    "                continue\n",
    "            except Exception:\n",
    "                # Container doesn't exist, try to create it\n",
    "                pass\n",
    "            \n",
    "            # Create the container\n",
    "            print(f\"üî® Creating container '{container_name}'...\")\n",
    "            container_client.create_container()\n",
    "            print(f\"‚úÖ Container '{container_name}' created successfully\")\n",
    "            created_containers.append(container_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with container '{container_name}': {e}\")\n",
    "            failed_containers.append((container_name, str(e)))\n",
    "            \n",
    "            # Additional diagnostics for authorization errors\n",
    "            if \"AuthorizationFailure\" in str(e):\n",
    "                print(f\"   üîç Authorization issue detected for '{container_name}'\")\n",
    "                print(f\"   This could be due to:\")\n",
    "                print(f\"   - Storage account access keys disabled\")\n",
    "                print(f\"   - Network access restrictions\")\n",
    "                print(f\"   - Storage account permissions\")\n",
    "    \n",
    "    print(f\"\\nüìä Container Creation Summary:\")\n",
    "    print(f\"   Successful: {len(created_containers)} - {created_containers}\")\n",
    "    print(f\"   Failed: {len(failed_containers)} - {[name for name, _ in failed_containers]}\")\n",
    "    \n",
    "    return len(failed_containers) == 0\n",
    "\n",
    "if blob_service_client:\n",
    "    print(\"üöÄ Running enhanced container creation...\")\n",
    "    success = create_containers_enhanced(blob_service_client)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Upload Functions\n",
    "The next cell creates a helpful DocumentUploader class that provides easy-to-use methods for uploading individual files or entire directories to Azure Blob Storage, complete with progress tracking and error handling to make document management seamless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document uploader initialized!\n"
     ]
    }
   ],
   "source": [
    "class DocumentUploader:\n",
    "    def __init__(self, blob_service_client):\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def upload_file(self, file_path: Path, container_name: str, blob_name: str = None) -> bool:\n",
    "        \"\"\"Upload a single file to blob storage\"\"\"\n",
    "        if blob_name is None:\n",
    "            blob_name = file_path.name\n",
    "            \n",
    "        try:\n",
    "            blob_client = self.blob_service_client.get_blob_client(\n",
    "                container=container_name, \n",
    "                blob=blob_name\n",
    "            )\n",
    "            \n",
    "            with open(file_path, 'rb') as data:\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "            print(f\"‚úÖ Uploaded: {file_path.name} ‚Üí {container_name}/{blob_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading {file_path.name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def upload_directory(self, directory_path: Path, container_name: str) -> Dict[str, bool]:\n",
    "        \"\"\"Upload all files from a directory to blob storage\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if not directory_path.exists():\n",
    "            print(f\"‚ùå Directory not found: {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        files = list(directory_path.glob('*'))\n",
    "        if not files:\n",
    "            print(f\"‚ÑπÔ∏è No files found in {directory_path}\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"üì§ Uploading {len(files)} files from {directory_path} to {container_name}...\")\n",
    "        \n",
    "        for file_path in tqdm(files, desc=\"Uploading files\"):\n",
    "            if file_path.is_file():\n",
    "                success = self.upload_file(file_path, container_name)\n",
    "                results[file_path.name] = success\n",
    "        \n",
    "        successful_uploads = sum(results.values())\n",
    "        print(f\"\\nüìä Upload Summary: {successful_uploads}/{len(results)} files uploaded successfully\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def list_blobs(self, container_name: str) -> List[str]:\n",
    "        \"\"\"List all blobs in a container\"\"\"\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "            blob_list = container_client.list_blobs()\n",
    "            return [blob.name for blob in blob_list]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error listing blobs in {container_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize uploader\n",
    "if blob_service_client:\n",
    "    uploader = DocumentUploader(blob_service_client)\n",
    "    print(\"‚úÖ Document uploader initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Documents to Blob Storage\n",
    "\n",
    "Separated by folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Uploading Policy Documents...\n",
      "==================================================\n",
      "üì§ Uploading 5 files from data/policies to policies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:00<00:00, 27.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: comprehensive_auto_policy.md ‚Üí policies/comprehensive_auto_policy.md\n",
      "‚úÖ Uploaded: motorcycle_policy.md ‚Üí policies/motorcycle_policy.md\n",
      "‚úÖ Uploaded: liability_only_policy.md ‚Üí policies/liability_only_policy.md\n",
      "‚úÖ Uploaded: commercial_auto_policy.md ‚Üí policies/commercial_auto_policy.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 28.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: high_value_vehicle_policy.md ‚Üí policies/high_value_vehicle_policy.md\n",
      "\n",
      "üìä Upload Summary: 5/5 files uploaded successfully\n",
      "\n",
      "üñºÔ∏è Uploading Claims Documents...\n",
      "==================================================\n",
      "üì§ Uploading 5 files from data/claims to claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash3.jpg ‚Üí claims/crash3.jpg\n",
      "‚úÖ Uploaded: crash5.jpg ‚Üí claims/crash5.jpg\n",
      "‚úÖ Uploaded: crash2.jpg ‚Üí claims/crash2.jpg\n",
      "‚úÖ Uploaded: crash1.jpg ‚Üí claims/crash1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 16.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash4.jpeg ‚Üí claims/crash4.jpeg\n",
      "\n",
      "üìä Upload Summary: 5/5 files uploaded successfully\n",
      "\n",
      "üìÑ Uploading Statements Documents...\n",
      "==================================================\n",
      "üì§ Uploading 5 files from data/statements to statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash2.md ‚Üí statements/crash2.md\n",
      "‚úÖ Uploaded: crash1.md ‚Üí statements/crash1.md\n",
      "‚úÖ Uploaded: crash4.md ‚Üí statements/crash4.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 31.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded: crash5.md ‚Üí statements/crash5.md\n",
      "‚úÖ Uploaded: crash3.md ‚Üí statements/crash3.md\n",
      "\n",
      "üìä Upload Summary: 5/5 files uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload policy documents\n",
    "print(\"üìÑ Uploading Policy Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "policy_results = uploader.upload_directory(Config.POLICIES_DIR, Config.POLICIES_CONTAINER)\n",
    "\n",
    "# Upload claims documents\n",
    "print(\"\\nüñºÔ∏è Uploading Claims Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "claims_results = uploader.upload_directory(Config.CLAIMS_DIR, Config.CLAIMS_CONTAINER)\n",
    "\n",
    "# Upload statements documents\n",
    "print(\"\\nüìÑ Uploading Statements Documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "statements_results = uploader.upload_directory(Config.STATEMENTS_DIR, Config.STATEMENTS_CONTAINER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Processing with Azure OpenAI GPT-4-1-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! As of this moment we have created 3 containers that have the data that we will use to our use case. Awesome! Now, it's time to process the data. We are currently handling `.md`and `.png` files. For such, we will create a class called `DocumentProcessor` that will have 2 key functions:\n",
    "- **process_markdown_for_vectorization** - will process the markdown files as normal text files for vectorization\n",
    "- **generate_image_description_with_gpt** - will use the multimodal capabilities of GPT-4.1-mini to process our image and give us a description. Later on, this will be really important for fraud analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, openai_client, blob_service_client):\n",
    "        self.openai_client = openai_client\n",
    "        self.blob_service_client = blob_service_client\n",
    "    \n",
    "    def get_blob_content(self, container_name: str, blob_name: str) -> bytes:\n",
    "        \"\"\"Download blob content as bytes\"\"\"\n",
    "        blob_client = self.blob_service_client.get_blob_client(\n",
    "            container=container_name, \n",
    "            blob=blob_name\n",
    "        )\n",
    "        blob_data = blob_client.download_blob()\n",
    "        return blob_data.readall()\n",
    "    \n",
    "    def encode_image_to_base64(self, image_bytes: bytes) -> str:\n",
    "        \"\"\"Encode image bytes to base64 string\"\"\"\n",
    "        return base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    def process_markdown_for_vectorization(self, container_name: str, blob_name: str) -> Dict:\n",
    "        \"\"\"Process markdown file for direct vectorization (no GPT processing)\"\"\"\n",
    "        try:\n",
    "            print(f\"üìÑ Preparing markdown for vectorization: {blob_name}...\")\n",
    "            \n",
    "            # Download and decode content\n",
    "            blob_content = self.get_blob_content(container_name, blob_name)\n",
    "            content = blob_content.decode('utf-8')\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"markdown\",\n",
    "                \"text_length\": len(content),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"processing_method\": \"direct_vectorization\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"text\": content,  # Original markdown content for vectorization\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"markdown\"}\n",
    "            }\n",
    "\n",
    "    def generate_image_description_with_gpt(self, container_name: str, blob_name: str) -> Dict:\n",
    "        try:\n",
    "            print(f\"üñºÔ∏è Generating description for image: {blob_name}...\")\n",
    "            \n",
    "            # Download image content\n",
    "            image_bytes = self.get_blob_content(container_name, blob_name)\n",
    "            base64_image = self.encode_image_to_base64(image_bytes)\n",
    "            \n",
    "            # Determine image format from file extension\n",
    "            file_extension = Path(blob_name).suffix.lower()\n",
    "            if file_extension == \".jpg\" or file_extension == \".jpeg\":\n",
    "                image_format = \"jpeg\"\n",
    "            elif file_extension == \".png\":\n",
    "                image_format = \"png\"\n",
    "            else:\n",
    "                image_format = \"jpeg\"  # default\n",
    "            \n",
    "            # Process with GPT-4.1-mini for description generation\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are an expert insurance claims analyst with advanced image analysis capabilities. \n",
    "                        Your task is to provide detailed, professional descriptions of insurance-related images, particularly vehicle damage and accident scenes.\n",
    "                        \n",
    "                        Focus on:\n",
    "                        - Type of vehicle and visible damage\n",
    "                        - Location and extent of damage (scratches, dents, broken parts, etc.)\n",
    "                        - Environmental context (road conditions, weather signs, location type)\n",
    "                        - Any visible people, other vehicles, or relevant objects\n",
    "                        - Overall severity assessment\n",
    "                        - Any safety concerns or hazards visible\n",
    "                        \n",
    "                        Provide clear, objective descriptions that would be useful for insurance claim processing and risk assessment.\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Please provide a detailed description of this insurance claim image. Focus on damage assessment, environmental factors, and any relevant details for insurance processing.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/{image_format};base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=4000,\n",
    "                temperature=0.3  # Slightly higher for more descriptive language\n",
    "            )\n",
    "            description = response.choices[0].message.content\n",
    "            \n",
    "            metadata = {\n",
    "                \"file_name\": blob_name,\n",
    "                \"container\": container_name,\n",
    "                \"file_type\": \"image\",\n",
    "                \"image_format\": image_format,\n",
    "                \"image_size_bytes\": len(image_bytes),\n",
    "                \"description_length\": len(description),\n",
    "                \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"model_used\": Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "                \"processing_type\": \"image_description\",\n",
    "                \"ready_for_embedding\": True\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"description\": description,  # Changed from \"text\" to \"description\"\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {blob_name}: {e}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"metadata\": {\"file_name\": blob_name, \"container\": container_name, \"file_type\": \"image\"}\n",
    "            }\n",
    "\n",
    "    def process_all_documents(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Process documents: prepare markdown for vectorization, generate descriptions for images\"\"\"\n",
    "        results = {\n",
    "            \"policies\": [],\n",
    "            \"claims\": [],\n",
    "            \"statements\": []  # Added statements to results\n",
    "        }\n",
    "        \n",
    "        # Process policy documents (markdown files) - prepare for vectorization only\n",
    "        print(\"üìÑ Preparing Policy Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        policy_blobs = uploader.list_blobs(Config.POLICIES_CONTAINER)\n",
    "        for blob_name in tqdm(policy_blobs, desc=\"Preparing policies\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.POLICIES_CONTAINER, blob_name)\n",
    "                results[\"policies\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process statements documents (markdown files) - prepare for vectorization only\n",
    "        print(\"\\nüìÑ Preparing Statements Documents for Vectorization...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        statements_blobs = uploader.list_blobs(Config.STATEMENTS_CONTAINER)\n",
    "        for blob_name in tqdm(statements_blobs, desc=\"Preparing statements\"):\n",
    "            if blob_name.endswith(\".md\"):\n",
    "                result = self.process_markdown_for_vectorization(Config.STATEMENTS_CONTAINER, blob_name)\n",
    "                results[\"statements\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-markdown file: {blob_name}\")\n",
    "        \n",
    "        # Process claims documents (images) - generate descriptions with GPT-4.1-mini \n",
    "        print(\"\\nüñºÔ∏è Generating Image Descriptions with GPT-4.1-mini ...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        claims_blobs = uploader.list_blobs(Config.CLAIMS_CONTAINER)\n",
    "        for blob_name in tqdm(claims_blobs, desc=\"Generating descriptions\"):\n",
    "            if blob_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                result = self.generate_image_description_with_gpt(Config.CLAIMS_CONTAINER, blob_name)\n",
    "                results[\"claims\"].append(result)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping non-image file: {blob_name}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_processed_results(self, results: Dict, output_file: str = \"processed_documents_for_vectorization.json\"):\n",
    "        \"\"\"Save processed results to JSON file and upload to blob storage\"\"\"\n",
    "        try:\n",
    "            # Save locally\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"üíæ Results saved locally: {output_file}\")\n",
    "            \n",
    "            # Upload to blob storage\n",
    "            success = uploader.upload_file(\n",
    "                Path(output_file), \n",
    "                Config.PROCESSED_CONTAINER, \n",
    "                output_file\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"‚òÅÔ∏è Results uploaded to blob storage: {Config.PROCESSED_CONTAINER}/{output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process All Documents with GPT-4.1-mini\n",
    "\n",
    "Now, let's seat and watch the magic happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processor initialized with GPT!\n",
      "\n",
      "üöÄ Starting document processing with GPT...\n",
      "============================================================\n",
      "üìÑ Preparing Policy Documents for Vectorization...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: commercial_auto_policy.md...\n",
      "üìÑ Preparing markdown for vectorization: comprehensive_auto_policy.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:00<00:00, 29.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: high_value_vehicle_policy.md...\n",
      "üìÑ Preparing markdown for vectorization: liability_only_policy.md...\n",
      "üìÑ Preparing markdown for vectorization: motorcycle_policy.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing policies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 31.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Preparing Statements Documents for Vectorization...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing statements:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: crash1.md...\n",
      "üìÑ Preparing markdown for vectorization: crash2.md...\n",
      "üìÑ Preparing markdown for vectorization: crash3.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing statements:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:00<00:00, 36.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing markdown for vectorization: crash4.md...\n",
      "üìÑ Preparing markdown for vectorization: crash5.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing statements: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 31.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñºÔ∏è Generating Image Descriptions with GPT-4.1-mini ...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash1.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  20%|‚ñà‚ñà        | 1/5 [00:08<00:34,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash2.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:22<00:35, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash3.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:35<00:24, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash4.jpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:55<00:15, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Generating description for image: crash5.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating descriptions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:09<00:00, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Document processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor with GPT\n",
    "if openai_client and blob_service_client:\n",
    "    processor = DocumentProcessor(openai_client, blob_service_client)\n",
    "    print(\"‚úÖ Document processor initialized with GPT!\")\n",
    "    \n",
    "    # Process documents using GPT\n",
    "    print(\"\\nüöÄ Starting document processing with GPT...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    processing_results = processor.process_all_documents()\n",
    "    \n",
    "    print(\"\\n‚úÖ Document processing completed!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize processor - missing clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Summary\n",
    "\n",
    "Perfect, now let's run the following code and we will be able to check locally the transcription of our files. Please do double check if they make sense. Special attention should be given to the information extracted from the images, as it should contain deeply detailed description of the crash in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved locally: processed_documents_for_vectorization.json\n",
      "‚úÖ Uploaded: processed_documents_for_vectorization.json ‚Üí processed-documents/processed_documents_for_vectorization.json\n",
      "‚òÅÔ∏è Results uploaded to blob storage: processed-documents/processed_documents_for_vectorization.json\n"
     ]
    }
   ],
   "source": [
    "# Save processing results\n",
    "processor.save_processed_results(processing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Cosmos our data!\n",
    "\n",
    "But first... if you inspected correctly you might have seen that we have indeed extracted data from our files, but it is not structured at all. We might as well do that! We will use the Claim_ID on the top part of each submission to create a database. And of course, to do that we will use... Generative AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "\n",
    "# Rename the model to something more appropriate\n",
    "class ClaimInfo(BaseModel):\n",
    "    claimant_id: str\n",
    "    policyholder_name: str\n",
    "    policyholder_address: str\n",
    "    policyholder_phone: str\n",
    "    policyholder_email: str\n",
    "    policy_number: str\n",
    "    vehicle_year_make_model: str\n",
    "    vehicle_color: str\n",
    "    vehicle_vin: str\n",
    "    vehicle_license_plate: str\n",
    "    incident_date: str\n",
    "    incident_time: str\n",
    "    incident_location: str\n",
    "    incident_description: str\n",
    "    damage_description: str\n",
    "    witness_name: str\n",
    "    witness_phone: str\n",
    "    police_department: str\n",
    "    police_report_number: str\n",
    "    repair_shop_name: str\n",
    "    repair_shop_address: str\n",
    "    attachments: str\n",
    "    claim_request: str\n",
    "    signature_name: str\n",
    "    signature_date: str\n",
    "\n",
    "\n",
    "def extract_structured_claim_info(text_content: str, claim_id: str) -> dict:\n",
    "    \"\"\"Extract structured information from claim text using Azure OpenAI structured outputs\"\"\"\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        \n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,  # Use your deployment name\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"\"\"You are an expert insurance claims processor. Extract structured information from crash statements and insurance claims. \n",
    "                    If any field is not available in the text, use \"N/A\" as the value. \n",
    "                    Be thorough and accurate in extracting all available information.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Extract the structured information from this crash statement for claim {claim_id}:\\n\\n{text_content}\"\n",
    "                },\n",
    "            ],\n",
    "            response_format=ClaimInfo,\n",
    "        )\n",
    "        \n",
    "        structured_data = completion.choices[0].message.parsed\n",
    "        print(f\"‚úÖ Extracted structured data for claim {claim_id}\")\n",
    "        return structured_data.model_dump()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting structured data for claim {claim_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting simplified crash report processing...\n",
      "üîç Extracting structured info for crash1 (Claim: CL001)...\n",
      "‚úÖ Extracted structured data for claim CL001\n",
      "üîç Extracting structured info for crash2 (Claim: CL002)...\n",
      "‚úÖ Extracted structured data for claim CL002\n",
      "üîç Extracting structured info for crash3 (Claim: CL003)...\n",
      "‚úÖ Extracted structured data for claim CL003\n",
      "üîç Extracting structured info for crash4 (Claim: CL001)...\n",
      "‚úÖ Extracted structured data for claim CL001\n",
      "üîç Extracting structured info for crash5 (Claim: CL004)...\n",
      "‚úÖ Extracted structured data for claim CL004\n",
      "‚úÖ Created simplified report for Claim ID: CL001 (2 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL002 (1 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL003 (1 crashes)\n",
      "‚úÖ Created simplified report for Claim ID: CL004 (1 crashes)\n",
      "\n",
      "üìä Created 4 simplified crash reports\n",
      "\n",
      "üìã Simplified Report Structure:\n",
      "  Claim CL001:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 2\n",
      "    - Policyholder: John Peterson, Policy: LIAB-AUTO-001\n",
      "  Claim CL002:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Samantha Turner, Policy: COMM-AUTO-001\n",
      "  Claim CL003:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Michael Rodriguez, Policy: LIAB-AUTO-001\n",
      "  Claim CL004:\n",
      "    - Structured Info: ‚úÖ\n",
      "    - Image Descriptions: 1\n",
      "    - Policyholder: Christopher J. Ryan, Policy: COMM-AUTO-001\n",
      "üíæ Saving simplified crash reports...\n",
      "‚úÖ Saved simplified Claim CL001 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: John Peterson\n",
      "      policy_number: LIAB-AUTO-001\n",
      "      incident_date: July 17, 2025\n",
      "      incident_location: Parking lot, 2325 Main Street, Springfield, OH 45503\n",
      "‚úÖ Saved simplified Claim CL002 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Samantha Turner\n",
      "      policy_number: COMM-AUTO-001\n",
      "      incident_date: July 18, 2025\n",
      "      incident_location: 2100 Madison Avenue, Albany, NY 12208\n",
      "‚úÖ Saved simplified Claim CL003 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Michael Rodriguez\n",
      "      policy_number: LIAB-AUTO-001\n",
      "      incident_date: July 19, 2025\n",
      "      incident_location: 400 East 8th Street, Harrisburg, PA 17102\n",
      "‚úÖ Saved simplified Claim CL004 to Cosmos DB\n",
      "   üìã Sample structured data:\n",
      "      policyholder_name: Christopher J. Ryan\n",
      "      policy_number: COMM-AUTO-001\n",
      "      incident_date: July 19, 2025\n",
      "      incident_location: Intersection of Karl Road & Bethel Road, Columbus, OH 43214\n",
      "üíæ Simplified crash reports saved to 'simplified_crash_reports.json'\n",
      "\n",
      "üìÑ Sample of simplified JSON structure:\n",
      "{\n",
      "  \"sample_claim\": {\n",
      "    \"claim_id\": \"CL001\",\n",
      "    \"structured_claim_info\": {\n",
      "      \"policyholder_name\": \"John Peterson\",\n",
      "      \"policy_number\": \"LIAB-AUTO-001\",\n",
      "      \"incident_date\": \"July 17, 2025\",\n",
      "      \"...\": \"all other structured fields\"\n",
      "    },\n",
      "    \"image_descriptions\": [\n",
      "      {\n",
      "        \"crash_number\": \"crash1\",\n",
      "        \"image_file\": \"crash1.jpg\",\n",
      "        \"description\": \"Detailed image description...\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def process_crash_reports_simplified():\n",
    "    \"\"\"Process JSON file and create simplified crash reports with only structured info and image descriptions\"\"\"\n",
    "    \n",
    "    # Load the processed JSON file\n",
    "    with open('processed_documents_for_vectorization.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    statements_lookup = {item['metadata']['file_name']: item for item in data['statements'] if item['success']}\n",
    "    claims_lookup = {item['metadata']['file_name']: item for item in data['claims'] if item['success']}\n",
    "    \n",
    "    # Define crash to claim ID mapping\n",
    "    crash_to_claim_mapping = {\n",
    "        'crash1': 'CL001',\n",
    "        'crash2': 'CL002', \n",
    "        'crash3': 'CL003',\n",
    "        'crash4': 'CL001',  # Same claim as crash1\n",
    "        'crash5': 'CL004'\n",
    "    }\n",
    "    \n",
    "    # Group by claim ID to handle multiple crashes per claim\n",
    "    claims_data = {}\n",
    "    \n",
    "    for crash_num, claim_id in crash_to_claim_mapping.items():\n",
    "        statement_file = f\"{crash_num}.md\"\n",
    "        image_files = [f\"{crash_num}.jpg\", f\"{crash_num}.jpeg\"]\n",
    "        \n",
    "        # Find matching image file\n",
    "        image_file = None\n",
    "        for img in image_files:\n",
    "            if img in claims_lookup:\n",
    "                image_file = img\n",
    "                break\n",
    "        \n",
    "        if statement_file in statements_lookup and image_file:\n",
    "            statement_data = statements_lookup[statement_file]\n",
    "            image_data = claims_lookup[image_file]\n",
    "            \n",
    "            if claim_id not in claims_data:\n",
    "                claims_data[claim_id] = {\n",
    "                    \"structured_info\": [],\n",
    "                    \"image_descriptions\": []\n",
    "                }\n",
    "            \n",
    "            # Extract structured information from the statement text\n",
    "            print(f\"üîç Extracting structured info for {crash_num} (Claim: {claim_id})...\")\n",
    "            structured_info = extract_structured_claim_info(statement_data['text'], claim_id)\n",
    "            \n",
    "            if structured_info:\n",
    "                claims_data[claim_id][\"structured_info\"].append({\n",
    "                    \"crash_number\": crash_num,\n",
    "                    \"structured_data\": structured_info\n",
    "                })\n",
    "            \n",
    "            # Add image description\n",
    "            claims_data[claim_id][\"image_descriptions\"].append({\n",
    "                \"crash_number\": crash_num,\n",
    "                \"image_file\": image_file,\n",
    "                \"description\": image_data['description']\n",
    "            })\n",
    "    \n",
    "    # Create simplified crash reports with only structured info and image descriptions\n",
    "    simplified_reports = []\n",
    "    for claim_id, claim_data in claims_data.items():\n",
    "        # Combine structured info from all crashes in this claim\n",
    "        combined_structured_info = {}\n",
    "        if claim_data[\"structured_info\"]:\n",
    "            # Use the first crash's structured info as base\n",
    "            combined_structured_info = claim_data[\"structured_info\"][0][\"structured_data\"].copy()\n",
    "            \n",
    "            # For claims with multiple crashes, add them as additional crashes\n",
    "            if len(claim_data[\"structured_info\"]) > 1:\n",
    "                combined_structured_info[\"additional_crashes\"] = []\n",
    "                for i in range(1, len(claim_data[\"structured_info\"])):\n",
    "                    additional_crash = {\n",
    "                        \"crash_number\": claim_data[\"structured_info\"][i][\"crash_number\"],\n",
    "                        \"structured_data\": claim_data[\"structured_info\"][i][\"structured_data\"]\n",
    "                    }\n",
    "                    combined_structured_info[\"additional_crashes\"].append(additional_crash)\n",
    "        \n",
    "        simplified_report = {\n",
    "            \"claim_id\": claim_id,\n",
    "            \"structured_claim_info\": combined_structured_info,\n",
    "            \"image_descriptions\": claim_data[\"image_descriptions\"]\n",
    "        }\n",
    "        \n",
    "        simplified_reports.append(simplified_report)\n",
    "        crashes_count = len(claim_data[\"structured_info\"])\n",
    "        print(f\"‚úÖ Created simplified report for Claim ID: {claim_id} ({crashes_count} crashes)\")\n",
    "    \n",
    "    return simplified_reports\n",
    "\n",
    "def save_simplified_cosmos_db(simplified_reports):\n",
    "    \"\"\"Save simplified reports to Cosmos DB\"\"\"\n",
    "    \n",
    "    # Initialize Cosmos client\n",
    "    client = CosmosClient(Config.COSMOS_ENDPOINT, Config.COSMOS_KEY)\n",
    "    database = client.create_database_if_not_exists(id=Config.COSMOS_DATABASE)\n",
    "    \n",
    "    # Update partition key to use claim_id\n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=Config.COSMOS_CONTAINER,\n",
    "        partition_key=PartitionKey(path=\"/claim_id\")\n",
    "    )\n",
    "    \n",
    "    # Save simplified reports\n",
    "    print(\"üíæ Saving simplified crash reports...\")\n",
    "    for report in simplified_reports:\n",
    "        try:\n",
    "            # Add required id field for Cosmos DB\n",
    "            report_with_id = report.copy()\n",
    "            report_with_id[\"id\"] = report[\"claim_id\"]\n",
    "            \n",
    "            container.upsert_item(body=report_with_id)\n",
    "            print(f\"‚úÖ Saved simplified Claim {report['claim_id']} to Cosmos DB\")\n",
    "            \n",
    "            # Print sample of structured info for verification\n",
    "            if report.get(\"structured_claim_info\"):\n",
    "                sample_fields = [\"policyholder_name\", \"policy_number\", \"incident_date\", \"incident_location\"]\n",
    "                print(f\"   üìã Sample structured data:\")\n",
    "                for field in sample_fields:\n",
    "                    value = report[\"structured_claim_info\"].get(field, \"N/A\")\n",
    "                    print(f\"      {field}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving {report['claim_id']}: {e}\")\n",
    "\n",
    "# Execute the simplified processing\n",
    "print(\"üöÄ Starting simplified crash report processing...\")\n",
    "simplified_reports = process_crash_reports_simplified()\n",
    "print(f\"\\nüìä Created {len(simplified_reports)} simplified crash reports\")\n",
    "\n",
    "# Display the simplified structure\n",
    "print(\"\\nüìã Simplified Report Structure:\")\n",
    "for report in simplified_reports:\n",
    "    print(f\"  Claim {report['claim_id']}:\")\n",
    "    print(f\"    - Structured Info: {'‚úÖ' if report.get('structured_claim_info') else '‚ùå'}\")\n",
    "    print(f\"    - Image Descriptions: {len(report.get('image_descriptions', []))}\")\n",
    "    \n",
    "    # Show sample structured data\n",
    "    if report.get(\"structured_claim_info\"):\n",
    "        policyholder = report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\")\n",
    "        policy_num = report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\")\n",
    "        print(f\"    - Policyholder: {policyholder}, Policy: {policy_num}\")\n",
    "\n",
    "# Save to Cosmos DB\n",
    "save_simplified_cosmos_db(simplified_reports)\n",
    "\n",
    "# Save the simplified reports locally\n",
    "with open('simplified_crash_reports.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_reports, f, indent=2, ensure_ascii=False)\n",
    "print(f\"üíæ Simplified crash reports saved to 'simplified_crash_reports.json'\")\n",
    "\n",
    "# Show a sample of what the final JSON looks like\n",
    "print(\"\\nüìÑ Sample of simplified JSON structure:\")\n",
    "if simplified_reports:\n",
    "    sample_report = simplified_reports[0]\n",
    "    print(json.dumps({\n",
    "        \"sample_claim\": {\n",
    "            \"claim_id\": sample_report[\"claim_id\"],\n",
    "            \"structured_claim_info\": {\n",
    "                \"policyholder_name\": sample_report[\"structured_claim_info\"].get(\"policyholder_name\", \"N/A\"),\n",
    "                \"policy_number\": sample_report[\"structured_claim_info\"].get(\"policy_number\", \"N/A\"),\n",
    "                \"incident_date\": sample_report[\"structured_claim_info\"].get(\"incident_date\", \"N/A\"),\n",
    "                \"...\": \"all other structured fields\"\n",
    "            },\n",
    "            \"image_descriptions\": [\n",
    "                {\n",
    "                    \"crash_number\": sample_report[\"image_descriptions\"][0][\"crash_number\"],\n",
    "                    \"image_file\": sample_report[\"image_descriptions\"][0][\"image_file\"],\n",
    "                    \"description\": \"Detailed image description...\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
